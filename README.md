# Deep-Learning
#### 01 - Activation Functions
          1. Sigmoid AF
          2. tanh AF
          3. ReLU (Rectified Linear Unit) AF
          4. Leaky ReLU AF
          5. ELU (Exponential Linear Unit) AF
          6. PReLU (Parametric Linear Unit) AF
          7. Swish AF
          8. Softplus AF
          9. Softmax AF


#### 02 - Weight Initialization Techniques
          1. Uniform Distribition
          2. Xavier/Gorat Distribution
              a. Xavier/Gorat Normal
              b. Xavier/Gorat Uniform
          3. He init
              a. He init Normal
              b. He init Uniform  
              
              
#### 03 - Optimizers
          1. Gradient Descent 
          2. Stochastic Gradient Descent
          3. Mini-batch Stochastic Gradient Descent
          4. Stochastic Gradient Descent with Momentum
          5. Adagrad-Adaptive Gradient Descent
          6. Adadelta
          7. RMSProp
          8. ADAM 


#### 04 - Loss/Cost Functions
      * For Regression:
          1. Squared Error Loss (MSE)
          2. Abosulute Error Loss (MAE)
          3. HUber Loss
      * For Classification:
          1. Sigmoid Cross Entropy Loss for binary classification
          2. Softmax Cross Entropy Loss for multiclass classification
#### 05 - Tensorflow
